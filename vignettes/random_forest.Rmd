---
title: 'randomForest model'
date: '`r Sys.Date()`'
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
    vignette: >
      %\VignetteIndexEntry{randomforest}
      %\VignetteEngine{knitr::rmarkdown}
      %\VignetteEncoding{UTF-8}
---

```{r knitr_opts, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.align = "center"
)
```

```{r init, echo = FALSE, message = FALSE, warning = FALSE}
library(RClusterCT)
library(ggplot2)
library(Matrix)
library(lattice)
library(randomForest)
library(caret)
library(mlbench)
library(e1071)
```

## Cell type prediction using Random Forest model

The goal is to build a random forest model using single-cell RNA-seq expression data to predict the cell type in each cluster.
The training set contains an expression matrix with normalized readcounts per gene per cell, clustering information and identified cell types. The test set contains only the expression and clusters. 
```{r, eval=F, echo=T}
# Reduce the expression matrix to only highly variable genes
# source('Reduce_matrix.R')

# Input files for random forest
# Training set
pbmc4 <- Reduce_matrix(pbmc4k_matrix, pbmc4k_meta, pbmc4k_vargenes)
# Test set
pbmc5 <- Reduce_matrix(pbmc5_matrix, pbmc5_meta, pbmc4k_vargenes)

# Make sure the listed genes (predictors) are the same in both dataset
names(pbmc5)[names(pbmc5) == "cluster"] <- "classified"
pbmc4.sub <- pbmc4[, colnames(pbmc4) %in% colnames(pbmc5)]
pbmc5.sub <- pbmc5[, colnames(pbmc5) %in% colnames(pbmc4.sub)]
```

### Plot training data 

```{r plot, fig.height=6, fig.width=8}
# plot tsne using known identities
qplot(tSNE_1, tSNE_2, colour = classified,  data = pbmc4k_meta)
```

### Model parameter tuning

```{r, eval=F, echo=T}
# Algorithm Tune (tuneRF)
seed <- 100
set.seed(seed)
# initial mtry <- sqrt(ncol(pbmc4))
x <- pbmc4[,2:ncol(pbmc4)]
y <- pbmc4[,1]
bestmtry <- tuneRF(x, y, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)
# In this case bestmtry is 256 when ntree is 500

# Tune model using caret
# Custom tuning
# https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/
customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

metric <- "Accuracy"
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry=c(250:260), .ntree=c(500, 1000, 1500, 2000))
set.seed(seed)
rf_tune <- train(classified~., data=pbmc4, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
summary(rf_tune)
plot(rf_tune)
```

### Build model with optimal parameters, and fit new data

```{r, eval=F, echo=T}
#Optimal mtry and ntree
bmtry<- 256
bntree<- 1000
#Build the model
rFmodel.sub <- randomForest(classified ~ ., data = pbmc4.sub, ntree = bntree, mtry = bmtry, importance = TRUE)
save(rFmodel.sub,file = "rFmodelsub.RData")
# To check important variables
importance(rFmodel)        
varImpPlot(rFmodel)
# Fit the model with new data
pred <- predict(rFmodel.sub, pbmc5.sub, type = "class")
# Checking classification accuracy
table(pred, pbmc5.sub$classified) 
```

